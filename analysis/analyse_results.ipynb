{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Neurocomputing/ESANN 2024\n",
    "\n",
    "This notebook contains the code to analyse the results of the \n",
    "Neurocomputing/ESANN 2024 paper, and it is responsible for generating\n",
    "the figures and tables in the paper.\n",
    "\n",
    "The notebook is organised as follows:\n",
    "\n",
    "1. The first section contains imports, constants, helper functions and load the \n",
    "    data.\n",
    "\n",
    "2. We show that the dict-wisard has competitive performance with the \n",
    "    classical machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. General constants, hhelper functions, and data loading\n",
    "\n",
    "Imports, global constants and packages' configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Union\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import json\n",
    "\n",
    "from utils import write_figure, write_latex_table, aggregate_mean_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "pd.set_option(\"display.float_format\", lambda x: \"%.4f\" % x)\n",
    "\n",
    "# ---------- Paths -------------\n",
    "# -- Inputs\n",
    "datasets_info_path = Path(\"datasets_info.json\")\n",
    "results_path = Path(\"results_wisard_folded.csv\")\n",
    "results_sklearn_path = Path(\"results_sklearn_folded.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read inputs and create a full dataframe\n",
    "\n",
    "1. Read the datasets specifications (`dataset_info`)\n",
    "2. Read the wisard results (`wisard_results`)\n",
    "3. Read the sklearn results (`sklearn_results`)\n",
    "4. Create a results dataframe, mergind dataset_info, wisard_results and sklearn_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>size</th>\n",
       "      <th>features</th>\n",
       "      <th>num_classes</th>\n",
       "      <th>train_size</th>\n",
       "      <th>test_size</th>\n",
       "      <th>balanced</th>\n",
       "      <th>metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>breast_cancer</td>\n",
       "      <td>141416</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>398</td>\n",
       "      <td>171</td>\n",
       "      <td>False</td>\n",
       "      <td>f1 weighted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dry_bean</td>\n",
       "      <td>1773910</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>10888</td>\n",
       "      <td>2723</td>\n",
       "      <td>False</td>\n",
       "      <td>f1 weighted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>glass</td>\n",
       "      <td>17413</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>149</td>\n",
       "      <td>65</td>\n",
       "      <td>False</td>\n",
       "      <td>f1 weighted</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset_name     size  features  num_classes  train_size  test_size  \\\n",
       "0  breast_cancer   141416        30            3         398        171   \n",
       "1       dry_bean  1773910        16            7       10888       2723   \n",
       "2          glass    17413         9           24         149         65   \n",
       "\n",
       "   balanced       metric  \n",
       "0     False  f1 weighted  \n",
       "1     False  f1 weighted  \n",
       "2     False  f1 weighted  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datasets information\n",
    "datasets_info = pd.read_json(datasets_info_path, orient=\"index\").reset_index(drop=True)\n",
    "datasets_info.rename(columns={\"name\": \"dataset_name\"}, inplace=True)\n",
    "datasets_info.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table written to: tables/datasets_info.tex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2513315/3004690570.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  info.loc[:, \"size\"] = info[\"size\"] / 1024\n",
      "/tmp/ipykernel_2513315/3004690570.py:16: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  latex_str = info.to_latex(\n"
     ]
    }
   ],
   "source": [
    "info = datasets_info[\n",
    "    [\"dataset_name\", \"features\", \"size\", \"num_classes\", \"balanced\"]\n",
    "]\n",
    "info.loc[:, \"size\"] = info[\"size\"] / 1024\n",
    "\n",
    "info = info.rename(\n",
    "    columns={\n",
    "        \"dataset_name\": \"Dataset\",\n",
    "        \"features\": \"Features\",\n",
    "        \"size\": \"Size (KB)\",\n",
    "        \"num_classes\": \"Classes\",\n",
    "        \"balanced\": \"Is Balanced?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "latex_str = info.to_latex(\n",
    "    index=False,\n",
    "    escape=True,\n",
    "    caption=\"Datasets information\",\n",
    "    label=\"tab:datasets_info\",\n",
    "    float_format=\"%.2f\",\n",
    ")\n",
    "\n",
    "write_latex_table(\"datasets_info.tex\", latex_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wisard results\n",
    "\n",
    "Read and parse wisard result to `wisard_results` dataframe.\n",
    "\n",
    "**Note**: The `wisard_results` already has aggregated results for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>fold</th>\n",
       "      <th>model</th>\n",
       "      <th>config_name</th>\n",
       "      <th>tuple_size</th>\n",
       "      <th>encoder</th>\n",
       "      <th>resolution</th>\n",
       "      <th>bleach</th>\n",
       "      <th>rams per discriminator</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy_std</th>\n",
       "      <th>f1</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>model_size</th>\n",
       "      <th>model_size_std</th>\n",
       "      <th>ties</th>\n",
       "      <th>ties_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6634</th>\n",
       "      <td>breast_cancer</td>\n",
       "      <td>3</td>\n",
       "      <td>Wisard</td>\n",
       "      <td>CountMinSketch (W: 436.0, D: 1.0)</td>\n",
       "      <td>12.0000</td>\n",
       "      <td>thermometer</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.5351</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4853</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>105600.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>yeast</td>\n",
       "      <td>1</td>\n",
       "      <td>Wisard</td>\n",
       "      <td>CountMinSketch (W: 40.0, D: 4.0)</td>\n",
       "      <td>24.0000</td>\n",
       "      <td>distributive-thermometer</td>\n",
       "      <td>48</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>0.5107</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.4790</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>104960.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>61.6667</td>\n",
       "      <td>3.2998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            dataset  fold   model                        config_name  \\\n",
       "6634  breast_cancer     3  Wisard  CountMinSketch (W: 436.0, D: 1.0)   \n",
       "1034          yeast     1  Wisard   CountMinSketch (W: 40.0, D: 4.0)   \n",
       "\n",
       "      tuple_size                   encoder  resolution  bleach  \\\n",
       "6634     12.0000               thermometer          12       1   \n",
       "1034     24.0000  distributive-thermometer          48       9   \n",
       "\n",
       "      rams per discriminator  accuracy  accuracy_std     f1  f1_std  \\\n",
       "6634                      30    0.5351        0.0000 0.4853  0.0000   \n",
       "1034                      16    0.5107        0.0136 0.4790  0.0168   \n",
       "\n",
       "      model_size  model_size_std    ties  ties_std  \n",
       "6634 105600.0000          0.0000 74.0000    0.0000  \n",
       "1034 104960.0000          0.0000 61.6667    3.2998  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_wisard_config_name(row) -> str:\n",
    "    \"\"\"Given a row, parse the name of configuration.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        The row of the dataframe.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The name of the configuration.\n",
    "    \"\"\"\n",
    "\n",
    "    names = []\n",
    "    if not pd.isna(row[\"num_hitters\"]):\n",
    "        names.append(f\"NR: {row['num_hitters']}\")\n",
    "    if not pd.isna(row[\"width\"]):\n",
    "        names.append(f\"W: {row['width']}\")\n",
    "    if not pd.isna(row[\"depth\"]):\n",
    "        names.append(f\"D: {row['depth']}\")\n",
    "    if not pd.isna(row[\"capacity\"]):\n",
    "        names.append(f\"C: {row['capacity']}\")\n",
    "    if not pd.isna(row[\"bucket_size\"]):\n",
    "        names.append(f\"BS: {row['bucket_size']}\")\n",
    "    if not pd.isna(row[\"threshold\"]):\n",
    "        names.append(f\"T: {row['threshold']}\")\n",
    "    if not pd.isna(row[\"est_elements\"]):\n",
    "        names.append(f\"EST: {row['est_elements']}\")\n",
    "    if not pd.isna(row[\"false_positive_rate\"]):\n",
    "        names.append(f\"FPR: {row['false_positive_rate']}\")\n",
    "\n",
    "    if names:\n",
    "        names = \", \".join(names)\n",
    "        return f\"{row['ram']} ({names})\"\n",
    "    else:\n",
    "        return row[\"ram\"]\n",
    "\n",
    "\n",
    "# --- Read results and add a column with the name of the configuration ---\n",
    "wisard_results = pd.read_csv(results_path).drop_duplicates()\n",
    "\n",
    "# --- Add useful columns ---\n",
    "wisard_results[\"tuple_size\"] = (\n",
    "    wisard_results[\"resolution\"] / wisard_results[\"tuple_resolution_factor\"]\n",
    ")\n",
    "wisard_results[\"config_name\"] = wisard_results.apply(\n",
    "    parse_wisard_config_name, axis=1\n",
    ")\n",
    "\n",
    "# --- Select the columns of interest ---\n",
    "wisard_results = wisard_results[\n",
    "    [\n",
    "        \"dataset_name\",\n",
    "        \"config_name\",\n",
    "        \"test_accuracy_mean\",\n",
    "        \"test_accuracy_std\",\n",
    "        \"test_f1 weighted_mean\",\n",
    "        \"test_f1 weighted_std\",\n",
    "        \"test_model size_mean\",\n",
    "        \"test_model size_std\",\n",
    "        \"test_ties_mean\",\n",
    "        \"test_ties_std\",\n",
    "        \"tuple_size\",\n",
    "        \"encoder\",\n",
    "        \"resolution\",\n",
    "        \"bleach\",\n",
    "        \"rams per discriminator\",\n",
    "        \"ram\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# --- Rename columns ---\n",
    "wisard_results = wisard_results.rename(\n",
    "    columns={\n",
    "        \"dataset_name\": \"dataset\",\n",
    "        \"ram\": \"model\",\n",
    "        \"test_ties_mean\": \"ties\",\n",
    "        \"test_ties_std\": \"ties_std\",\n",
    "        \"test_accuracy_mean\": \"accuracy\",\n",
    "        \"test_accuracy_std\": \"accuracy_std\",\n",
    "        \"test_f1 weighted_mean\": \"f1\",\n",
    "        \"test_f1 weighted_std\": \"f1_std\",\n",
    "        \"test_model size_mean\": \"model_size\",\n",
    "        \"test_model size_std\": \"model_size_std\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- Add model column and drop duplicates ---\n",
    "wisard_results[\"model\"] = \"Wisard\"\n",
    "wisard_results.drop_duplicates(inplace=True)\n",
    "\n",
    "# Split dataset name from fold\n",
    "wisard_results[[\"dataset\", \"fold\"]] = wisard_results[\"dataset\"].str.split(\n",
    "    \"_fold_\", expand=True\n",
    ")\n",
    "wisard_results[\"fold\"] = wisard_results[\"fold\"].astype(int)\n",
    "\n",
    "\n",
    "# Rearange columns\n",
    "wisard_results = wisard_results[[\n",
    "    \"dataset\",\n",
    "    \"fold\",\n",
    "    \"model\",\n",
    "    \"config_name\",\n",
    "    \"tuple_size\",\n",
    "    \"encoder\",\n",
    "    \"resolution\",\n",
    "    \"bleach\",\n",
    "    \"rams per discriminator\",\n",
    "    \"accuracy\",\n",
    "    \"accuracy_std\",\n",
    "    \"f1\",\n",
    "    \"f1_std\",\n",
    "    \"model_size\",\n",
    "    \"model_size_std\",\n",
    "    \"ties\",\n",
    "    \"ties_std\",\n",
    "]]\n",
    "\n",
    "wisard_results.sample(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy_std</th>\n",
       "      <th>f1</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>model_size</th>\n",
       "      <th>model_size_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>breast_cancer</td>\n",
       "      <td>Wisard</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>61675.6667</td>\n",
       "      <td>64519.6794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dry_bean</td>\n",
       "      <td>Wisard</td>\n",
       "      <td>0.9027</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.9028</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>968603.5333</td>\n",
       "      <td>443718.5251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>glass</td>\n",
       "      <td>Wisard</td>\n",
       "      <td>0.6842</td>\n",
       "      <td>0.0509</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>53000.8667</td>\n",
       "      <td>22996.4997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image_segmentation</td>\n",
       "      <td>Wisard</td>\n",
       "      <td>0.8556</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.8526</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>142505.2000</td>\n",
       "      <td>46475.5446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iris</td>\n",
       "      <td>Wisard</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>2949.1333</td>\n",
       "      <td>3307.9763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>letter</td>\n",
       "      <td>Wisard</td>\n",
       "      <td>0.8677</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.8686</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>3516032.8333</td>\n",
       "      <td>1340785.4729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rice</td>\n",
       "      <td>Wisard</td>\n",
       "      <td>0.9178</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.9176</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>46699.2000</td>\n",
       "      <td>29841.7810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>satimage</td>\n",
       "      <td>Wisard</td>\n",
       "      <td>0.8904</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.8864</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>6291516.6000</td>\n",
       "      <td>4400154.0757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>segment</td>\n",
       "      <td>Wisard</td>\n",
       "      <td>0.8730</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.8653</td>\n",
       "      <td>0.0186</td>\n",
       "      <td>131163.3333</td>\n",
       "      <td>34125.9199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>vehicle</td>\n",
       "      <td>Wisard</td>\n",
       "      <td>0.8850</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.8836</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>372700.8667</td>\n",
       "      <td>221038.1812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>wine</td>\n",
       "      <td>Wisard</td>\n",
       "      <td>0.9603</td>\n",
       "      <td>0.0320</td>\n",
       "      <td>0.9601</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>97341.2667</td>\n",
       "      <td>56457.3154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>yeast</td>\n",
       "      <td>Wisard</td>\n",
       "      <td>0.5589</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.5447</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>727397.8000</td>\n",
       "      <td>226269.2917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               dataset   model  accuracy  accuracy_std     f1  f1_std  \\\n",
       "0        breast_cancer  Wisard    0.9537        0.0179 0.9537  0.0177   \n",
       "1             dry_bean  Wisard    0.9027        0.0040 0.9028  0.0035   \n",
       "2                glass  Wisard    0.6842        0.0509 0.6333  0.0481   \n",
       "3   image_segmentation  Wisard    0.8556        0.0181 0.8526  0.0223   \n",
       "4                 iris  Wisard    0.9800        0.0277 0.9800  0.0277   \n",
       "5               letter  Wisard    0.8677        0.0262 0.8686  0.0258   \n",
       "6                 rice  Wisard    0.9178        0.0145 0.9176  0.0146   \n",
       "7             satimage  Wisard    0.8904        0.0052 0.8864  0.0058   \n",
       "8              segment  Wisard    0.8730        0.0140 0.8653  0.0186   \n",
       "9              vehicle  Wisard    0.8850        0.0195 0.8836  0.0197   \n",
       "10                wine  Wisard    0.9603        0.0320 0.9601  0.0323   \n",
       "11               yeast  Wisard    0.5589        0.0077 0.5447  0.0046   \n",
       "\n",
       "     model_size  model_size_std  \n",
       "0    61675.6667      64519.6794  \n",
       "1   968603.5333     443718.5251  \n",
       "2    53000.8667      22996.4997  \n",
       "3   142505.2000      46475.5446  \n",
       "4     2949.1333       3307.9763  \n",
       "5  3516032.8333    1340785.4729  \n",
       "6    46699.2000      29841.7810  \n",
       "7  6291516.6000    4400154.0757  \n",
       "8   131163.3333      34125.9199  \n",
       "9   372700.8667     221038.1812  \n",
       "10   97341.2667      56457.3154  \n",
       "11  727397.8000     226269.2917  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_wisard_results = wisard_results[(wisard_results[\"config_name\"] == \"Dict\")]\n",
    "\n",
    "lines = []\n",
    "for (dataset, fold), dataset_df in dict_wisard_results.groupby([\"dataset\", \"fold\"]):\n",
    "    metric_name = datasets_info.loc[datasets_info[\"dataset_name\"] == dataset, \"metric\"].iloc[0]\n",
    "    if metric_name == \"f1 weighted\":\n",
    "        metric_name = \"f1\"\n",
    "    line = dataset_df.sort_values(by=metric_name, ascending=False).iloc[0]\n",
    "    lines.append(line)\n",
    "\n",
    "dict_wisard_results = pd.DataFrame(lines)\n",
    "\n",
    "dict_wisard_results = aggregate_mean_std(\n",
    "    dict_wisard_results, \n",
    "    group_by=[\"dataset\"],\n",
    "    keys_to_aggregate=[\"accuracy\", \"f1\", \"model_size\"]\n",
    ")\n",
    "\n",
    "dict_wisard_results[\"model\"] = \"Wisard\"\n",
    "\n",
    "# Rearange columns\n",
    "dict_wisard_results = dict_wisard_results[[\n",
    "    \"dataset\",\n",
    "    \"model\",\n",
    "    \"accuracy\",\n",
    "    \"accuracy_std\",\n",
    "    \"f1\",\n",
    "    \"f1_std\",\n",
    "    \"model_size\",\n",
    "    \"model_size_std\",\n",
    "]]\n",
    "\n",
    "dict_wisard_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit Learn results\n",
    "\n",
    "Read and parse sklearn result to `sklearn_results` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy_std</th>\n",
       "      <th>f1</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>model_size</th>\n",
       "      <th>model_size_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>glass</td>\n",
       "      <td>KNN-5</td>\n",
       "      <td>0.6359</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.6070</td>\n",
       "      <td>0.0411</td>\n",
       "      <td>29866.0000</td>\n",
       "      <td>71.5542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>satimage</td>\n",
       "      <td>MLP-3L</td>\n",
       "      <td>0.9112</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.9106</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>596230.4667</td>\n",
       "      <td>246.3741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset   model  accuracy  accuracy_std     f1  f1_std  model_size  \\\n",
       "17     glass   KNN-5    0.6359        0.0453 0.6070  0.0411  29866.0000   \n",
       "69  satimage  MLP-3L    0.9112        0.0055 0.9106  0.0057 596230.4667   \n",
       "\n",
       "    model_size_std  \n",
       "17         71.5542  \n",
       "69        246.3741  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read sklearn results and aggregate multiple runs\n",
    "sklearn_results = pd.read_csv(results_sklearn_path).drop_duplicates()\n",
    "\n",
    "# Aggregate metric for multiple runs\n",
    "sklearn_results = aggregate_mean_std(\n",
    "    df=sklearn_results,\n",
    "    group_by=[\n",
    "        \"model\",\n",
    "        \"model kwargs\",\n",
    "        \"dataset name\",\n",
    "        \"experiment name\",\n",
    "    ],\n",
    "    keys_to_aggregate=[\n",
    "        \"accuracy\",\n",
    "        \"f1 weighted\",\n",
    "        \"f1 macro\",\n",
    "        \"f1 micro\",\n",
    "        \"train time\",\n",
    "        \"predict time\",\n",
    "        \"model size\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Select columns of interest\n",
    "sklearn_results = sklearn_results[\n",
    "    [\n",
    "        \"dataset name\",\n",
    "        \"model\",\n",
    "        \"model kwargs\",\n",
    "        \"accuracy\",\n",
    "        \"accuracy_std\",\n",
    "        \"f1 weighted\",\n",
    "        \"f1 weighted_std\",\n",
    "        \"model size\",\n",
    "        \"model size_std\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Rename columns\n",
    "sklearn_results.rename(\n",
    "    columns={\n",
    "        \"dataset name\": \"dataset\",\n",
    "        \"model kwargs\": \"config_name\",\n",
    "        \"f1 weighted\": \"f1\",\n",
    "        \"f1 weighted_std\": \"f1_std\",\n",
    "        \"model size\": \"model_size\",\n",
    "        \"model size_std\": \"model_size_std\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "def transform_config_to_model_name(row):\n",
    "    config = json.loads(row[\"config_name\"])\n",
    "    if row[\"model\"] == \"knn\":\n",
    "        row[\"model\"] = f\"KNN-{config['n_neighbors']}\"\n",
    "    elif row[\"model\"] == \"mlp\":\n",
    "        row[\"model\"] = f\"MLP-{len(config['hidden_layer_sizes'])}L\"\n",
    "    elif row[\"model\"] == \"random-forest\":\n",
    "        row[\"model\"] = \"Random Forest\"\n",
    "    elif row[\"model\"] == \"svm\":\n",
    "        row[\"model\"] = f\"SVM-{config.get('kernel', 'rbf')}\"\n",
    "        \n",
    "    row = row.drop(\"config_name\")\n",
    "    return row\n",
    "        \n",
    "\n",
    "# Transform config to model name\n",
    "sklearn_results = sklearn_results.apply(transform_config_to_model_name, axis=1)\n",
    "\n",
    "# Split dataset name from fold\n",
    "sklearn_results[['dataset', 'fold']] = sklearn_results['dataset'].str.split('_fold_', expand=True)\n",
    "sklearn_results['fold'] = sklearn_results['fold'].astype(int)\n",
    "\n",
    "# Aggregate folds\n",
    "n_folds = sklearn_results[\"fold\"].nunique()\n",
    "old_len = len(sklearn_results)\n",
    "\n",
    "sklearn_results = aggregate_mean_std(\n",
    "    df=sklearn_results,\n",
    "    group_by=[\"model\", \"dataset\"],\n",
    "    keys_to_aggregate=[\"accuracy\", \"f1\", \"model_size\"]\n",
    ")\n",
    "\n",
    "# Check if the aggregation was correct\n",
    "assert len(sklearn_results) == old_len / n_folds, f\"Expected {old_len / n_folds} got {len(sklearn_results)}\"\n",
    "\n",
    "# Rearange columns\n",
    "sklearn_results = sklearn_results[[\n",
    "    \"dataset\",\n",
    "    \"model\",\n",
    "    \"accuracy\",\n",
    "    \"accuracy_std\",\n",
    "    \"f1\",\n",
    "    \"f1_std\",\n",
    "    \"model_size\",\n",
    "    \"model_size_std\",\n",
    "]]\n",
    "\n",
    "sklearn_results.sample(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy_std</th>\n",
       "      <th>f1</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>model_size</th>\n",
       "      <th>model_size_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>breast_cancer</td>\n",
       "      <td>KNN-10</td>\n",
       "      <td>0.9332</td>\n",
       "      <td>0.0237</td>\n",
       "      <td>0.9325</td>\n",
       "      <td>0.0248</td>\n",
       "      <td>113565.6000</td>\n",
       "      <td>110.9090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>breast_cancer</td>\n",
       "      <td>KNN-5</td>\n",
       "      <td>0.9315</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.9308</td>\n",
       "      <td>0.0307</td>\n",
       "      <td>113565.6000</td>\n",
       "      <td>110.9090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>breast_cancer</td>\n",
       "      <td>MLP-1L</td>\n",
       "      <td>0.9251</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.9245</td>\n",
       "      <td>0.0424</td>\n",
       "      <td>83856.0667</td>\n",
       "      <td>755.9721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>breast_cancer</td>\n",
       "      <td>MLP-2L</td>\n",
       "      <td>0.9045</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.9037</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>325137.5333</td>\n",
       "      <td>385.8725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>breast_cancer</td>\n",
       "      <td>MLP-3L</td>\n",
       "      <td>0.9087</td>\n",
       "      <td>0.0396</td>\n",
       "      <td>0.9082</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>567566.0000</td>\n",
       "      <td>260.4160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>yeast</td>\n",
       "      <td>MLP-3L</td>\n",
       "      <td>0.5224</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.5161</td>\n",
       "      <td>0.0876</td>\n",
       "      <td>539396.8667</td>\n",
       "      <td>143.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>yeast</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.5386</td>\n",
       "      <td>0.1013</td>\n",
       "      <td>0.5329</td>\n",
       "      <td>0.0957</td>\n",
       "      <td>10770556.8000</td>\n",
       "      <td>60068.1367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>yeast</td>\n",
       "      <td>SVM-poly</td>\n",
       "      <td>0.5148</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.5146</td>\n",
       "      <td>0.0901</td>\n",
       "      <td>131641.0000</td>\n",
       "      <td>1303.5797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>yeast</td>\n",
       "      <td>SVM-rbf</td>\n",
       "      <td>0.5215</td>\n",
       "      <td>0.1006</td>\n",
       "      <td>0.5186</td>\n",
       "      <td>0.0847</td>\n",
       "      <td>141197.0000</td>\n",
       "      <td>1395.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>yeast</td>\n",
       "      <td>Wisard</td>\n",
       "      <td>0.5589</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.5447</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>727397.8000</td>\n",
       "      <td>226269.2917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           dataset          model  accuracy  accuracy_std     f1  f1_std  \\\n",
       "0    breast_cancer         KNN-10    0.9332        0.0237 0.9325  0.0248   \n",
       "1    breast_cancer          KNN-5    0.9315        0.0294 0.9308  0.0307   \n",
       "2    breast_cancer         MLP-1L    0.9251        0.0420 0.9245  0.0424   \n",
       "3    breast_cancer         MLP-2L    0.9045        0.0294 0.9037  0.0300   \n",
       "4    breast_cancer         MLP-3L    0.9087        0.0396 0.9082  0.0406   \n",
       "..             ...            ...       ...           ...    ...     ...   \n",
       "127          yeast         MLP-3L    0.5224        0.0954 0.5161  0.0876   \n",
       "128          yeast  Random Forest    0.5386        0.1013 0.5329  0.0957   \n",
       "129          yeast       SVM-poly    0.5148        0.1030 0.5146  0.0901   \n",
       "130          yeast        SVM-rbf    0.5215        0.1006 0.5186  0.0847   \n",
       "131          yeast         Wisard    0.5589        0.0077 0.5447  0.0046   \n",
       "\n",
       "       model_size  model_size_std  \n",
       "0     113565.6000        110.9090  \n",
       "1     113565.6000        110.9090  \n",
       "2      83856.0667        755.9721  \n",
       "3     325137.5333        385.8725  \n",
       "4     567566.0000        260.4160  \n",
       "..            ...             ...  \n",
       "127   539396.8667        143.0914  \n",
       "128 10770556.8000      60068.1367  \n",
       "129   131641.0000       1303.5797  \n",
       "130   141197.0000       1395.0914  \n",
       "131   727397.8000     226269.2917  \n",
       "\n",
       "[132 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge results\n",
    "results_df = pd.concat([dict_wisard_results, sklearn_results])\n",
    "results_df = results_df.sort_values(by=[\"dataset\", \"model\"]).reset_index(drop=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy_std</th>\n",
       "      <th>f1</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>model_size</th>\n",
       "      <th>model_size_std</th>\n",
       "      <th>metric</th>\n",
       "      <th>metric_std</th>\n",
       "      <th>performance_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>sepsis</td>\n",
       "      <td>KNN-10</td>\n",
       "      <td>0.9258</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5978355.2000</td>\n",
       "      <td>28.6217</td>\n",
       "      <td>0.8909</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>rice</td>\n",
       "      <td>MLP-1L</td>\n",
       "      <td>0.5364</td>\n",
       "      <td>0.1536</td>\n",
       "      <td>0.4540</td>\n",
       "      <td>0.1729</td>\n",
       "      <td>26696.6667</td>\n",
       "      <td>151.5994</td>\n",
       "      <td>0.4540</td>\n",
       "      <td>0.1729</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>wine</td>\n",
       "      <td>KNN-10</td>\n",
       "      <td>0.7076</td>\n",
       "      <td>0.0441</td>\n",
       "      <td>0.6960</td>\n",
       "      <td>0.0560</td>\n",
       "      <td>34795.6000</td>\n",
       "      <td>122.6899</td>\n",
       "      <td>0.6960</td>\n",
       "      <td>0.0560</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>wine</td>\n",
       "      <td>MLP-2L</td>\n",
       "      <td>0.6704</td>\n",
       "      <td>0.1173</td>\n",
       "      <td>0.6156</td>\n",
       "      <td>0.1384</td>\n",
       "      <td>289444.8000</td>\n",
       "      <td>1228.3786</td>\n",
       "      <td>0.6156</td>\n",
       "      <td>0.1384</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>breast_cancer</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.9637</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.9635</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>313211.1333</td>\n",
       "      <td>20666.8041</td>\n",
       "      <td>0.9635</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>iris</td>\n",
       "      <td>Wisard</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>2949.1333</td>\n",
       "      <td>3307.9763</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>accuracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>glass</td>\n",
       "      <td>SVM-poly</td>\n",
       "      <td>0.3551</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.1862</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>21343.6000</td>\n",
       "      <td>63.5358</td>\n",
       "      <td>0.1862</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dry_bean</td>\n",
       "      <td>MLP-3L</td>\n",
       "      <td>0.4578</td>\n",
       "      <td>0.0765</td>\n",
       "      <td>0.4071</td>\n",
       "      <td>0.0744</td>\n",
       "      <td>549375.5333</td>\n",
       "      <td>215.6137</td>\n",
       "      <td>0.4071</td>\n",
       "      <td>0.0744</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>letter</td>\n",
       "      <td>Wisard</td>\n",
       "      <td>0.8677</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.8686</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>3516032.8333</td>\n",
       "      <td>1340785.4729</td>\n",
       "      <td>0.8677</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>accuracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>vehicle</td>\n",
       "      <td>MLP-3L</td>\n",
       "      <td>0.8334</td>\n",
       "      <td>0.0465</td>\n",
       "      <td>0.8324</td>\n",
       "      <td>0.0486</td>\n",
       "      <td>544003.2667</td>\n",
       "      <td>140.3314</td>\n",
       "      <td>0.8334</td>\n",
       "      <td>0.0465</td>\n",
       "      <td>accuracy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dataset          model  accuracy  accuracy_std     f1  f1_std  \\\n",
       "97          sepsis         KNN-10    0.9258        0.0007 0.8909  0.0001   \n",
       "72            rice         MLP-1L    0.5364        0.1536 0.4540  0.1729   \n",
       "114           wine         KNN-10    0.7076        0.0441 0.6960  0.0560   \n",
       "117           wine         MLP-2L    0.6704        0.1173 0.6156  0.1384   \n",
       "5    breast_cancer  Random Forest    0.9637        0.0141 0.9635  0.0144   \n",
       "44            iris         Wisard    0.9800        0.0277 0.9800  0.0277   \n",
       "24           glass       SVM-poly    0.3551        0.0101 0.1862  0.0093   \n",
       "13        dry_bean         MLP-3L    0.4578        0.0765 0.4071  0.0744   \n",
       "53          letter         Wisard    0.8677        0.0262 0.8686  0.0258   \n",
       "109        vehicle         MLP-3L    0.8334        0.0465 0.8324  0.0486   \n",
       "\n",
       "      model_size  model_size_std  metric  metric_std performance_metric  \n",
       "97  5978355.2000         28.6217  0.8909      0.0001                 f1  \n",
       "72    26696.6667        151.5994  0.4540      0.1729                 f1  \n",
       "114   34795.6000        122.6899  0.6960      0.0560                 f1  \n",
       "117  289444.8000       1228.3786  0.6156      0.1384                 f1  \n",
       "5    313211.1333      20666.8041  0.9635      0.0144                 f1  \n",
       "44     2949.1333       3307.9763  0.9800      0.0277           accuracy  \n",
       "24    21343.6000         63.5358  0.1862      0.0093                 f1  \n",
       "13   549375.5333        215.6137  0.4071      0.0744                 f1  \n",
       "53  3516032.8333    1340785.4729  0.8677      0.0262           accuracy  \n",
       "109  544003.2667        140.3314  0.8334      0.0465           accuracy  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add metric column based on dataset info\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for _, row in datasets_info.iterrows():\n",
    "    df = results_df[results_df[\"dataset\"] == row[\"dataset_name\"]].copy()\n",
    "    if row[\"metric\"] == \"f1 weighted\":\n",
    "        metric = \"f1\"\n",
    "        metric_std = \"f1_std\"\n",
    "    else:\n",
    "        metric = \"accuracy\"\n",
    "        metric_std = \"accuracy_std\"\n",
    "    \n",
    "    df[\"metric\"] = df[metric]\n",
    "    df[\"metric_std\"] = df[metric_std]\n",
    "    df[\"performance_metric\"] = metric\n",
    "    dfs.append(df.reset_index(drop=True))\n",
    "\n",
    "results_df = pd.concat(dfs).reset_index(drop=True)\n",
    "results_df.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter results\n",
    "base_results = base_results[base_results[\"model\"].isin([\"wisard\", \"knn\", \"random-forest\", \"svm\", \"mlp\"])]\n",
    "base_results = base_results[~base_results[\"dataset\"].isin([\"mnist\", \"olivetti\", \"sensorless_drive\"])]\n",
    "# base_results = base_results[~base_results[\"dataset\"].isin([\"olivetti\", \"sensorless_drive\"])]\n",
    "base_results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Wisard has competitive results with SKLearn\n",
    "\n",
    "Here we show that the dict-wisard has competitive performance with the\n",
    "classical machine learning algorithms.\n",
    "\n",
    "To do that, we plot the accuracy of the wisard and sklearn algorithms for each\n",
    "dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read results and filter bloom filter results\n",
    "results = base_results.copy()\n",
    "results = results[\n",
    "    (results[\"model\"] != \"wisard\") | (results[\"config_name\"] == \"Dict\")\n",
    "]\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the best performance for each dataset and model\n",
    "best_metric_df = (\n",
    "    results.groupby([\"dataset\", \"model\"])\n",
    "    .apply(lambda group: group.loc[group[\"metric\"].idxmax()])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "best_metric_df.value_counts(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'results' DataFrame with columns: 'dataset', 'model', 'accuracy'\n",
    "\n",
    "# Create a grouped bar chart for accuracy per model and dataset\n",
    "fig_grouped_bar = px.bar(\n",
    "    best_metric_df,\n",
    "    x=\"dataset\",\n",
    "    y=\"metric\",\n",
    "    error_y=\"metric_std\",\n",
    "    color=\"model\",\n",
    "    #  title='Metric Comparison by Model and Dataset',\n",
    "    labels={\"metric\": \"Performance\", \"dataset\": \"Dataset\", \"model\": \"\"},\n",
    "    barmode=\"group\",\n",
    "    color_discrete_sequence=px.colors.qualitative.Prism,\n",
    ")\n",
    "\n",
    "\n",
    "# Display the plot\n",
    "fig_grouped_bar.update_layout(\n",
    "    legend=dict(\n",
    "        orientation=\"h\", yanchor=\"top\", y=1.20, xanchor=\"center\", x=0.5\n",
    "    ),\n",
    "    height=400,\n",
    "    width=2480 / 2.5,\n",
    "    font=dict(family=\"Times New Roman\", size=14),\n",
    ")\n",
    "\n",
    "write_figure(\"models_performance.pdf\", fig_grouped_bar)\n",
    "fig_grouped_bar.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'results' DataFrame with columns: 'dataset', 'model', 'accuracy'\n",
    "\n",
    "# Create a grouped horizontal bar chart for accuracy per model and dataset with reversed bar groups\n",
    "fig_grouped_bar = px.bar(\n",
    "    best_metric_df,\n",
    "    y=\"dataset\",\n",
    "    x=\"metric\",\n",
    "    error_x=\"metric_std\",\n",
    "    color=\"model\",\n",
    "    #  title='Metric Comparison by Model and Dataset',\n",
    "    labels={\"metric\": \"Performance\", \"dataset\": \"Dataset\", \"model\": \"\"},\n",
    "    barmode=\"group\",\n",
    "    orientation='h',\n",
    "    color_discrete_sequence=px.colors.qualitative.Prism,\n",
    ")\n",
    "\n",
    "\n",
    "# Reverse the order of the bar groups\n",
    "fig_grouped_bar.update_layout(\n",
    "    yaxis=dict(autorange=\"reversed\"),\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "fig_grouped_bar.update_layout(\n",
    "    legend=dict(\n",
    "        orientation=\"h\", yanchor=\"top\", y=1.05, xanchor=\"center\", x=0.5\n",
    "    ),\n",
    "    height=1200,\n",
    "    width=2480 / 4,\n",
    "    font=dict(family=\"Times New Roman\", size=14),\n",
    ")\n",
    "\n",
    "write_figure(\"models_performance_horizontal.pdf\", fig_grouped_bar)\n",
    "fig_grouped_bar.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many times per dataset, wisard is the best model\n",
    "\n",
    "print(\"How many times per dataset, each model is the best?\")\n",
    "best_metric_df.loc[\n",
    "    best_metric_df.groupby(\"dataset\")[\"metric\"].idxmax()\n",
    "].value_counts(\"model\").to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Wisard has competitive results with SKLearn and is smallest\n",
    "\n",
    "Here we show that costing up to 2% of performance, the dict-wisard is much\n",
    "smaller than the sklearn algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Up to 2% of accuracy loss\n",
    "metric_threshold = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read results and filter bloom filter results\n",
    "results = base_results.copy()\n",
    "results = results[\n",
    "    (results[\"model\"] != \"wisard\") | (results[\"config_name\"] == \"Dict\")\n",
    "]\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the best performance for each dataset and model\n",
    "best_metric_dataset_model = (\n",
    "    results.groupby([\"dataset\", \"model\"])\n",
    "    .apply(lambda group: group.loc[group[\"metric\"].idxmax()])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "best_metric_dataset_model.value_counts(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the model_size_ratio column. This column is the ratio between the model\n",
    "# size of each model and the model size of the best model for each dataset\n",
    "temp = []\n",
    "\n",
    "for dset_name, dset_df in best_metric_dataset_model.groupby(\"dataset\"):\n",
    "    best_row = dset_df.sort_values(by=\"metric\", ascending=False).iloc[0]\n",
    "    dset_df[\"model_size_ratio\"] = dset_df[\"model_size\"] / best_row[\"model_size\"]\n",
    "    # Min max normalization\n",
    "    dset_df[\"normalized_model_size_ratio\"] = (\n",
    "        dset_df[\"model_size_ratio\"] - dset_df[\"model_size_ratio\"].min()\n",
    "    ) / (dset_df[\"model_size_ratio\"].max() - dset_df[\"model_size_ratio\"].min())\n",
    "    dset_df[\"best_tradeoff\"] = False\n",
    "\n",
    "    best_tradeoff = (\n",
    "        dset_df[dset_df[\"metric\"] >= best_row[\"metric\"] - metric_threshold]\n",
    "        .sort_values(by=\"normalized_model_size_ratio\", ascending=True)\n",
    "        .iloc[0]\n",
    "    )\n",
    "    dset_df.loc[best_tradeoff.name, \"best_tradeoff\"] = True\n",
    "\n",
    "    temp.append(dset_df)\n",
    "\n",
    "best_metric_dataset_model = pd.concat(temp)\n",
    "best_metric_dataset_model.head(n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 100)\n",
    "dfs = {}\n",
    "for dset_name, dset_df in best_metric_dataset_model.groupby(\"dataset\"):\n",
    "    dset_df[\"model_size\"] = (dset_df[\"model_size\"] / 1024)\n",
    "    max_val = dset_df[\"metric\"].max()\n",
    "    max_size =  dset_df[\"model_size\"].max()\n",
    "    dset_df[\"relative performance\"] = dset_df[\"metric\"]  / max_val\n",
    "    dset_df[\"relative size\"] = dset_df[\"model_size\"]  / max_size\n",
    "    # dset_df.index = dset_df[\"model\"]\n",
    "    dset_df = dset_df[[\"model\", \"metric\",  \"model_size\", \"relative performance\", \"relative size\", \"accuracy\", \"f1\"]]\n",
    "    dfs[dset_name] = dset_df\n",
    "    \n",
    "result_df = pd.concat(dfs.values(), keys=dfs.keys())\n",
    "result_df.reset_index(level=0, inplace=True)\n",
    "result_df = result_df.rename(columns={\"level_0\": \"dataset\"})\n",
    "result_df.to_csv(\"temp.csv\", index=False)\n",
    "print(f\"Results written to temp.csv\")\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pivot the DataFrame to create the raw metric table\n",
    "# table_df_raw = result_df.pivot(index='dataset', columns='model', values='metric')\n",
    "\n",
    "# # Pivot the DataFrame to create the relative performance metric table\n",
    "# table_df_relative_performance = result_df.pivot(index='dataset', columns='model', values='relative performance')\n",
    "\n",
    "# # Join the two tables based on dataset\n",
    "# joined_df = table_df_raw.join(table_df_relative_performance, lsuffix=\"_raw\", rsuffix=\"_relative_performance\")\n",
    "\n",
    "\n",
    "# # joined_df.to_csv(\"temp.csv\", index=True)\n",
    "\n",
    "# # joined_df = joined_df.reset_index()\n",
    "\n",
    "# joined_df = joined_df.rename_axis(None, axis=1).reset_index()\n",
    "# # joined_df.index = range(len(joined_df))\n",
    "\n",
    "# # joined_df.columns = joined_df.columns.to_list()\n",
    "\n",
    "\n",
    "# raw_df = joined_df[['dataset', 'knn_raw', 'mlp_raw', 'random-forest_raw', 'svm_raw', 'wisard_raw']]\n",
    "# relative_df = joined_df[['dataset', 'knn_relative_performance', 'mlp_relative_performance', 'random-forest_relative_performance', 'svm_relative_performance', 'wisard_relative_performance']]\n",
    "\n",
    "# raw_df[\"dataset\"] = raw_df[\"dataset\"].str.replace(\"_\", \" \")\n",
    "# relative_df[\"dataset\"] = relative_df[\"dataset\"].str.replace(\"_\", \" \")\n",
    "# raw_df.columns = raw_df.columns.str.replace(\"-\", \" \")\n",
    "# relative_df.columns = relative_df.columns.str.replace(\"-\", \" \")\n",
    "\n",
    "# line = {\"dataset\": \"Mean\"}\n",
    "# for c in raw_df.columns:\n",
    "#     if c != \"dataset\":\n",
    "#         line[c] = raw_df[c].mean()\n",
    "# raw_df.loc[len(raw_df)] = line\n",
    "\n",
    "# line = {\"dataset\": \"Mean\"}\n",
    "# for c in relative_df.columns:\n",
    "#     if c != \"dataset\":\n",
    "#         line[c] = relative_df[c].mean()\n",
    "# relative_df.loc[len(relative_df)] = line\n",
    "\n",
    "\n",
    "# # Setting the dataset column as the index\n",
    "# raw_df.set_index('dataset', inplace=True)\n",
    "# relative_df.set_index('dataset', inplace=True)\n",
    "\n",
    "# # # Concatenating the DataFrames\n",
    "# final_df = pd.concat([raw_df, relative_df], axis=1)\n",
    "\n",
    "# # # Renaming the columns\n",
    "# final_df.columns = pd.MultiIndex.from_product([['Raw', 'Relative'], raw_df.columns.str.split('_').str[0]])\n",
    "\n",
    "# write_latex_table(\"performance_table.tex\", final_df.to_latex(float_format=\"%.2f\"))\n",
    "# final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mean_line(df):\n",
    "    line = {\"dataset\": \"Mean\"}\n",
    "    for c in df.columns:\n",
    "        if c != \"dataset\":\n",
    "            line[c] = df[c].mean()\n",
    "    df.loc[len(df)] = line\n",
    "    return df\n",
    "\n",
    "def raw_relative_table(df, raw_metric, relative_metric):\n",
    "    # Pivot the DataFrame to create the raw metric table and relative table\n",
    "    raw_df = (\n",
    "        df.pivot(index=\"dataset\", columns=\"model\", values=raw_metric)\n",
    "        .rename_axis(None, axis=1)\n",
    "        .reset_index()\n",
    "    )\n",
    "    raw_df[\"dataset\"] = raw_df[\"dataset\"].str.replace(\"_\", \" \")\n",
    "    raw_df.columns = raw_df.columns.str.replace(\"-\", \" \")\n",
    "    raw_df = raw_df[[\"dataset\", \"svm\", \"mlp\", \"knn\", \"random forest\", \"wisard\"]]\n",
    "    raw_df = add_mean_line(raw_df)\n",
    "    raw_df.set_index(\"dataset\", inplace=True)\n",
    "\n",
    "    relative_df = (\n",
    "        df.pivot(\n",
    "            index=\"dataset\", columns=\"model\", values=relative_metric\n",
    "        )\n",
    "        .rename_axis(None, axis=1)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    relative_df[\"dataset\"] = relative_df[\"dataset\"].str.replace(\"_\", \" \")\n",
    "    relative_df.columns = relative_df.columns.str.replace(\"-\", \" \")\n",
    "    relative_df = relative_df[[\"dataset\", \"svm\", \"mlp\", \"knn\", \"random forest\", \"wisard\"]]\n",
    "    relative_df = add_mean_line(relative_df)\n",
    "    relative_df.set_index(\"dataset\", inplace=True)\n",
    "    \n",
    "    # Concatenating the DataFrames\n",
    "    final_df = pd.concat([raw_df, relative_df], axis=1)\n",
    "\n",
    "    final_df.columns = pd.MultiIndex.from_product(\n",
    "        [[\"Absolute\", \"Relative\"], raw_df.columns.str.split(\"_\").str[0]]\n",
    "    )\n",
    "    return final_df\n",
    "\n",
    "performance_df = raw_relative_table(result_df.copy(), \"metric\", \"relative performance\")\n",
    "order_of_datasets = performance_df[\"Relative\"][\"wisard\"].sort_values(ascending=False).keys().to_list()\n",
    "order_of_datasets.remove(\"Mean\")\n",
    "order_of_datasets.append(\"Mean\")\n",
    "performance_df.index = order_of_datasets\n",
    "write_latex_table(\"performance_table.tex\", performance_df.to_latex(float_format=\"%.2f\"))\n",
    "\n",
    "size_df = raw_relative_table(result_df.copy(), \"model_size\", \"relative size\")\n",
    "size_df.index = order_of_datasets\n",
    "write_latex_table(\"size_table.tex\", size_df.to_latex(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = []\n",
    "\n",
    "# def belongs_to_pareto(df, model):\n",
    "#     metric = df[df[\"model\"] == model][\"metric\"].iloc[0]\n",
    "#     size = df[df[\"model\"] == model][\"model_size\"].iloc[0]\n",
    "    \n",
    "#     for r_index, row in df.iterrows():\n",
    "#         if row[\"metric\"] > metric and row[\"model_size\"] < size:\n",
    "#             return False\n",
    "        \n",
    "#     return True\n",
    "    \n",
    "# columns = result_df[\"dataset\"].unique()\n",
    "    \n",
    "\n",
    "# models = [\"wisard\", \"random-forest\", \"svm\", \"mlp\", \"knn\"]\n",
    "# for c in columns:\n",
    "#     x_df = result_df[result_df[\"dataset\"] == c]\n",
    "#     if belongs_to_pareto(x_df, \"wisard\"):\n",
    "#         print(f\"Wisard belongs to pareto in {c}\")\n",
    "#         # x_df.index = x_df.index.str.upper()\n",
    "    \n",
    "#     lines = []\n",
    "#     for m in models:\n",
    "#         line = x_df[x_df[\"model\"] == m]\n",
    "#         if belongs_to_pareto(x_df, m):\n",
    "#             print(f\"Wisard belongs to pareto in {c}\")\n",
    "#             line[\"pareto\"] = True\n",
    "#         else:\n",
    "#             line[\"pareto\"] = False\n",
    "#         lines.append(line)\n",
    "        \n",
    "        \n",
    "#     x_df = pd.concat(lines)\n",
    "    \n",
    "#     dfs.append(x_df)\n",
    "    \n",
    "# n = pd.concat(dfs).reset_index()\n",
    "    \n",
    "# # n[\"dataset\"] = n[\"level_0\"]\n",
    "# # n[\"pareto\"] = n[\"pareto\"].astype(int)\n",
    "# n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assume 'results' DataFrame with columns: 'dataset', 'model', 'accuracy'\n",
    "\n",
    "# # Create a grouped horizontal bar chart for accuracy per model and dataset with reversed bar groups\n",
    "# fig_grouped_bar = px.bar(\n",
    "#     n,\n",
    "#     y=\"dataset\",\n",
    "#     x=\"metric\",\n",
    "#     # error_x=\"metric_std\",\n",
    "#     color=\"model\",\n",
    "#     #  title='Metric Comparison by Model and Dataset',\n",
    "#     labels={\"metric\": \"Performance\", \"dataset\": \"Dataset\", \"model\": \"\"},\n",
    "#     barmode=\"group\",\n",
    "#     orientation='h',\n",
    "#     color_discrete_sequence=px.colors.qualitative.Prism,\n",
    "# )\n",
    "\n",
    "\n",
    "# # Reverse the order of the bar groups\n",
    "# fig_grouped_bar.update_layout(\n",
    "#     yaxis=dict(autorange=\"reversed\"),\n",
    "# )\n",
    "\n",
    "# # Display the plot\n",
    "# fig_grouped_bar.update_layout(\n",
    "#     legend=dict(\n",
    "#         orientation=\"h\", yanchor=\"top\", y=1.05, xanchor=\"center\", x=0.5\n",
    "#     ),\n",
    "#     height=1200,\n",
    "#     width=2480 / 4,\n",
    "#     font=dict(family=\"Times New Roman\", size=14),\n",
    "# )\n",
    "\n",
    "# write_figure(\"models_performance_horizontal.pdf\", fig_grouped_bar)\n",
    "# fig_grouped_bar.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Pareto frontier\n",
    "def is_pareto_efficient(costs):\n",
    "    is_efficient = np.ones(costs.shape[0], dtype=bool)\n",
    "    for i, c in enumerate(costs):\n",
    "        if is_efficient[i]:\n",
    "            is_efficient[is_efficient] = np.any(costs[is_efficient] < c, axis=1)\n",
    "            is_efficient[i] = True  # Keep the current point\n",
    "    return is_efficient\n",
    "\n",
    "for dset_name, dset_df in result_df.groupby(\"dataset\"):\n",
    "    costs = dset_df[[\"model_size\", \"metric\"]].to_numpy()\n",
    "    # Invert metric (lower is better)\n",
    "    costs[:, 1] = 1 / costs[:, 1]\n",
    "    pareto = is_pareto_efficient(costs)\n",
    "    result_df.loc[dset_df.index, \"pareto\"] = pareto\n",
    "    \n",
    "result_df.to_csv(\"temp.csv\", index=False)\n",
    "print(f\"Csv written to temp.csv\")\n",
    "result_df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.groupby(\"model\").pareto.value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming 'data' is our DataFrame\n",
    "data = result_df.copy()\n",
    "\n",
    "# dfs = []\n",
    "# for dset in order_of_datasets:\n",
    "#     x = data[data[\"dataset\"] == dset].copy()\n",
    "#     dfs.append(x)\n",
    "# data = pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "# Define marker symbols for each model\n",
    "marker_symbols = {\n",
    "    \"knn\": \"circle\",\n",
    "    \"mlp\": \"square\",\n",
    "    \"random-forest\": \"diamond\",\n",
    "    \"svm\": \"cross\",\n",
    "    \"wisard\": \"x\",\n",
    "}\n",
    "\n",
    "# Define model names for legend\n",
    "model_names = {\n",
    "    \"knn\": \"KNN\",\n",
    "    \"mlp\": \"MLP\",\n",
    "    \"random-forest\": \"Random Forest\",\n",
    "    \"svm\": \"SVM\",\n",
    "    \"wisard\": \"Wisard\",\n",
    "}\n",
    "\n",
    "pareto_colors = {\n",
    "    True: px.colors.qualitative.Plotly[1],\n",
    "    False: px.colors.qualitative.Plotly[0],\n",
    "}\n",
    "\n",
    "# data[\"relative size\"] = np.log(data[\"relative size\"])\n",
    "\n",
    "fig = px.scatter(\n",
    "    data,\n",
    "    x=\"relative size\",\n",
    "    y=\"metric\",\n",
    "    symbol=\"model\",\n",
    "    symbol_map=marker_symbols,\n",
    "    color=\"pareto\",\n",
    "    color_discrete_map=pareto_colors,\n",
    "    facet_col=\"dataset\",\n",
    "    facet_col_wrap=3,\n",
    "    height=1000,\n",
    "    width=900,\n",
    "    facet_row_spacing=0.03,\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    marker=dict(size=7),\n",
    "    selector=dict(mode=\"markers\"),\n",
    "    showlegend=False,  # Hides the legend entries created by Plotly Express\n",
    ")\n",
    "\n",
    "for anno in fig[\"layout\"][\"annotations\"]:\n",
    "    anno[\"text\"] = anno[\"text\"].split(\"=\")[1].replace(\"_\", \" \")\n",
    "\n",
    "# Manually map symbols to names in the legend\n",
    "legend_labels = {\n",
    "    symbol: model_names[model] for model, symbol in marker_symbols.items()\n",
    "}\n",
    "\n",
    "# Create a custom legend\n",
    "custom_legend = []\n",
    "for symbol, model_name in legend_labels.items():\n",
    "    custom_legend.append(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(\n",
    "                symbol=symbol, size=12, color=px.colors.qualitative.Plotly[0]\n",
    "            ),\n",
    "            name=model_name,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Add custom legend to the figure\n",
    "for trace in custom_legend:\n",
    "    fig.add_trace(trace)\n",
    "\n",
    "fig.update_layout(\n",
    "    legend=dict(\n",
    "        title=\"\",  # Set title to empty string to remove the legend title\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"top\",\n",
    "        y=1.07,\n",
    "        xanchor=\"center\",\n",
    "        x=0.5,\n",
    "        traceorder=\"normal\",  # Set trace order to normal to arrange legend entries horizontally\n",
    "    ),\n",
    "    margin=dict(l=10, r=10, t=10, b=10),\n",
    "    font=dict(family=\"Times New Roman\", size=14),\n",
    ")\n",
    "\n",
    "write_figure(\"model_metric_size.pdf\", fig)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming 'data' is our DataFrame\n",
    "data = result_df.copy()\n",
    "\n",
    "# Define marker symbols for each model\n",
    "marker_symbols = {\n",
    "    \"knn\": \"circle\",\n",
    "    \"mlp\": \"square\",\n",
    "    \"random-forest\": \"diamond\",\n",
    "    \"svm\": \"cross\",\n",
    "    \"wisard\": \"x\",\n",
    "}\n",
    "\n",
    "# Define model names for legend\n",
    "model_names = {\n",
    "    \"knn\": \"KNN\",\n",
    "    \"mlp\": \"MLP\",\n",
    "    \"random-forest\": \"Random Forest\",\n",
    "    \"svm\": \"SVM\",\n",
    "    \"wisard\": \"Wisard\",\n",
    "}\n",
    "\n",
    "pareto_colors = {\n",
    "    True: px.colors.qualitative.Plotly[1],\n",
    "    False: px.colors.qualitative.Plotly[0],\n",
    "}\n",
    "\n",
    "\n",
    "rows = 5\n",
    "cols = 3\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data[\"relative size\"] = np.log(data[\"relative size\"])\n",
    "\n",
    "fig = px.scatter(\n",
    "    data,\n",
    "    x=\"relative size\",\n",
    "    y=\"metric\",\n",
    "    symbol=\"model\",\n",
    "    symbol_map=marker_symbols,\n",
    "    color=\"pareto\",\n",
    "    color_discrete_map=pareto_colors,\n",
    "    facet_col=\"dataset\",\n",
    "    facet_col_wrap=3,\n",
    "    height=1000,\n",
    "    width=900,\n",
    "    facet_row_spacing=0.03,\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    marker=dict(size=7),\n",
    "    selector=dict(mode=\"markers\"),\n",
    "    showlegend=False,  # Hides the legend entries created by Plotly Express\n",
    ")\n",
    "\n",
    "for anno in fig[\"layout\"][\"annotations\"]:\n",
    "    anno[\"text\"] = anno[\"text\"].split(\"=\")[1].replace(\"_\", \" \")\n",
    "\n",
    "# Manually map symbols to names in the legend\n",
    "legend_labels = {\n",
    "    symbol: model_names[model] for model, symbol in marker_symbols.items()\n",
    "}\n",
    "\n",
    "# Create a custom legend\n",
    "custom_legend = []\n",
    "for symbol, model_name in legend_labels.items():\n",
    "    custom_legend.append(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(\n",
    "                symbol=symbol, size=12, color=px.colors.qualitative.Plotly[0]\n",
    "            ),\n",
    "            name=model_name,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Add custom legend to the figure\n",
    "for trace in custom_legend:\n",
    "    fig.add_trace(trace)\n",
    "\n",
    "fig.update_layout(\n",
    "    legend=dict(\n",
    "        title=\"\",  # Set title to empty string to remove the legend title\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"top\",\n",
    "        y=1.07,\n",
    "        xanchor=\"center\",\n",
    "        x=0.5,\n",
    "        traceorder=\"normal\",  # Set trace order to normal to arrange legend entries horizontally\n",
    "    ),\n",
    "    margin=dict(l=10, r=10, t=10, b=10),\n",
    "    font=dict(family=\"Times New Roman\", size=14),\n",
    ")\n",
    "\n",
    "write_figure(\"model_metric_size.pdf\", fig)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for dset_name, dset_df in best_metric_dataset_model.groupby(\"dataset\"):\n",
    "    max_val = dset_df[\"metric\"].max()\n",
    "    dset_df[\"model_size (KB)\"] = (dset_df[\"model_size\"] / 1024)\n",
    "    dset_df[\"relative performance\"] = dset_df[\"metric\"]  / max_val\n",
    "    dset_df = dset_df[[\"relative performance\", \"model_size (KB)\"]]\n",
    "    dfs[dset_name] = dset_df\n",
    "    \n",
    "result_df = pd.concat(dfs.values(), keys=dfs.keys()).reset_index()\n",
    "# result_df.reset_index(level=0, inplace=True)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"How many times per dataset, each model is the best tradeoff?\")\n",
    "best_metric_dataset_model[\n",
    "    best_metric_dataset_model.best_tradeoff == True\n",
    "].value_counts(\"model\").to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facets = 3\n",
    "cmap = px.colors.qualitative.Prism\n",
    "\n",
    "colors = {\n",
    "    name: cmap[i]\n",
    "    for i, name in enumerate(sorted(best_metric_dataset_model.model.unique()))\n",
    "}\n",
    "\n",
    "# Scatter plot for trade-off with normalized model size\n",
    "fig_tradeoff_normalized = px.scatter(\n",
    "    best_metric_dataset_model,\n",
    "    x=\"metric\",\n",
    "    y=\"normalized_model_size_ratio\",\n",
    "    color=\"model\",\n",
    "    facet_col=\"dataset\",\n",
    "    facet_col_wrap=facets,\n",
    "    # title=\"Trade-off Between Metric and Normalized Model Size Across Datasets\",\n",
    "    labels={\n",
    "        \"metric\": \"Performance\",\n",
    "        \"normalized_model_size_ratio\": \"Size Ratio (normalized)\",\n",
    "        \"model\": \"\",\n",
    "    },\n",
    "    facet_row_spacing=0.07,\n",
    "    width=1400,\n",
    "    height=800,\n",
    "    color_discrete_sequence=cmap,\n",
    ")\n",
    "\n",
    "\n",
    "fig_tradeoff_normalized.update_traces(\n",
    "    marker=dict(size=7.5),\n",
    ")\n",
    "\n",
    "fig_tradeoff_normalized.update_xaxes(showticklabels=True)\n",
    "\n",
    "fig_tradeoff_normalized.update_yaxes(showticklabels=True)\n",
    "\n",
    "fig_tradeoff_normalized.for_each_annotation(\n",
    "    lambda a: a.update(text=a.text.split(\"=\")[-1])\n",
    ")\n",
    "\n",
    "# fig_tradeoff_normalized.update_layout(\n",
    "#     legend=dict(\n",
    "#         orientation=\"h\", yanchor=\"bottom\", y=-0.15, xanchor=\"center\", x=0.5,\n",
    "#     ),\n",
    "\n",
    "# )\n",
    "\n",
    "fig_tradeoff_normalized.update_layout(\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"top\",\n",
    "        y=1.05,\n",
    "        xanchor=\"center\",\n",
    "        x=0.5,\n",
    "        itemsizing=\"constant\",  # Set this to \"constant\" to show only one item for color\n",
    "        font=dict(family=\"Times New Roman\", size=14),\n",
    "    ),\n",
    "    height=1200,\n",
    "    width=2480 /2.5,\n",
    "    font=dict(family=\"Times New Roman\", size=14),\n",
    ")\n",
    "num_rows = len(results.dataset.unique()) // facets\n",
    "num_cols = facets\n",
    "# datasets = list(sorted(results.dataset.unique(), reverse=False))\n",
    "\n",
    "for r in range(num_rows):\n",
    "    for c in range(num_cols):\n",
    "       \n",
    "        facet = r * num_cols + c\n",
    "        dset = fig_tradeoff_normalized.layout.annotations[facet]['text']\n",
    "\n",
    "        x_line = (\n",
    "            results[results[\"dataset\"] == dset][\"metric\"].max()\n",
    "            - metric_threshold\n",
    "        )\n",
    "        \n",
    " # Scatter plot for best tradeoff points with a cross\n",
    "        best_tradeoff_points = best_metric_dataset_model[\n",
    "            (best_metric_dataset_model[\"dataset\"] == dset)\n",
    "            & (best_metric_dataset_model[\"best_tradeoff\"] == True)\n",
    "        ]\n",
    "        \n",
    "        best_tradeoff_model = best_tradeoff_points.model.iloc[0]\n",
    "        \n",
    "        # print(f\"Facet {facet}: {dset} - {best_tradeoff_model}. Max: {results[results['dataset'] == dset]['metric'].max()}, line: {x_line}\")\n",
    "        \n",
    "        fig_tradeoff_normalized.add_trace(\n",
    "            go.Scatter(\n",
    "                x=best_tradeoff_points[\"metric\"],\n",
    "                y=best_tradeoff_points[\"normalized_model_size_ratio\"],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(\n",
    "                    size=10,\n",
    "                    symbol=\"x\",\n",
    "                    color=colors[best_tradeoff_model]\n",
    "                ),\n",
    "                showlegend=False,  # To not duplicate in the legend\n",
    "            ),\n",
    "            row=r + 1,\n",
    "            col=c + 1,\n",
    "        )\n",
    "        \n",
    "        fig_tradeoff_normalized.add_vline(\n",
    "            x=x_line, line_dash=\"dot\", row=r + 1, col=c + 1, line_width=1\n",
    "        )\n",
    "\n",
    "# Display the plot\n",
    "fig_tradeoff_normalized.show()\n",
    "\n",
    "write_figure(\"performance_size_tradeoff_normalized.pdf\", fig_tradeoff_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metric_dataset_model[\n",
    "    best_metric_dataset_model.best_tradeoff == True\n",
    "][[\"dataset\", \"model\", \"config_name\", \"metric\", \"model_size\", \"model_size_ratio\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metric_dataset_model[best_metric_dataset_model [\"dataset\"] == \"iris\"].dropna(axis=1)[[\"model\", \"metric\", \"model_size\", \"model_size_ratio\", \"normalized_model_size_ratio\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metric_dataset_model[[\"dataset\", \"model\", \"config_name\", \"metric\", \"model_size\", \"model_size_ratio\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metric_dataset_model[\n",
    "    (best_metric_dataset_model.best_tradeoff == True) & (best_metric_dataset_model.model == \"wisard\")\n",
    "][\"model_size_ratio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_info = best_metric_dataset_model[best_metric_dataset_model.model == \"wisard\"]\n",
    "encoder_info = encoder_info[[\"dataset\", \"encoder\", \"resolution\", \"tuple_size\", \"bleach\"]].reset_index(drop=True)\n",
    "encoder_info.rename(columns={\"encoder\": \"Encoder\", \"resolution\": \"Resolution\", \"tuple_size\": \"Tuple Size\", \"bleach\": \"Bleach\"}, inplace=True)\n",
    "encoder_info[\"Encoder\"] = encoder_info[\"Encoder\"].apply(lambda x: \"Distributive Thermometer\" if x == \"distributive-thermometer\" else \"Thermometer\")\n",
    "encoder_info[\"Resolution\"] = encoder_info[\"Resolution\"].astype(int)\n",
    "encoder_info[\"Tuple Size\"] = encoder_info[\"Tuple Size\"].astype(int)\n",
    "encoder_info[\"Bleach\"] = encoder_info[\"Bleach\"].astype(int)\n",
    "encoder_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_str = encoder_info.to_latex(\n",
    "    index=False,\n",
    "    escape=True,\n",
    "    caption=\"Parameters used for each experiment\",\n",
    "    label=\"tab:experiment-parameters\",\n",
    "    float_format=\"%.2f\",\n",
    ")\n",
    "write_latex_table(\"experiment_parameters.tex\", latex_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Select the wisard with best bloom filter\n",
    "\n",
    "Here we show that, costing up to 1% of performance of the best dict-wisard, a \n",
    "space-efficient bloom filter achieves the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Up to 1% of accuracy loss\n",
    "metric_threshold = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = base_results.copy()\n",
    "best_results = best_metric_dataset_model.copy()\n",
    "\n",
    "results = results[results[\"model\"] == \"wisard\"]\n",
    "best_results = best_results[best_results[\"model\"] == \"wisard\"]\n",
    "\n",
    "results[\"bloom-filter\"] = results[\"config_name\"].apply(lambda x: x.split(\" \")[0])\n",
    "best_results[\"bloom-filter\"] = best_results[\"config_name\"].apply(lambda x: x.split(\" \")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "\n",
    "\n",
    "for dset_name, dset_df in results.groupby(\"dataset\"):\n",
    "    best_model = best_results[best_results[\"dataset\"] == dset_name].iloc[0]\n",
    "    # print(f\"*** Dataset: {dset_name} with metric: {best_model['metric']} and model size: {int(best_model['model_size'])}\")\n",
    "    bests = dset_df[dset_df[\"metric\"] >= best_model[\"metric\"] - metric_threshold]\n",
    "    bests[\"metric_improvement\"] = bests[\"metric\"]/ best_model[\"metric\"]\n",
    "    bests[\"model_size_improvement\"] = bests[\"model_size\"]/ best_model[\"model_size\"]\n",
    "    bests[\"best_metric\"]  = best_model[\"metric\"]\n",
    "    bests[\"best_model_size\"]  = best_model[\"model_size\"]\n",
    "    temp.append(bests)\n",
    "    \n",
    "results = pd.concat(temp, ignore_index=True)\n",
    "results.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results = results.loc[results.groupby(\"dataset\")[\"model_size_improvement\"].idxmin()]\n",
    "best_results[\"bloom-filter\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results_table = best_results.sort_values(by=[\"dataset\"])[[\"dataset\", \"config_name\", \"model_size_improvement\"]]\n",
    "\n",
    "best_results_table.rename(\n",
    "    columns={\n",
    "        \"dataset\": \"Dataset\",\n",
    "        \"config_name\": \"Bloom Filter\",\n",
    "        \"model_size_improvement\": \"Size Ratio\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "best_results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_str = best_results_table.to_latex(\n",
    "    index=False,\n",
    "    escape=True,\n",
    "    caption=\"Best Bloom Filter configuration for each dataset\",\n",
    "    label=\"tab:best_bloom_filter\",\n",
    "    float_format=\"%.2f\",\n",
    ")\n",
    "\n",
    "write_latex_table(\"best_bloom_filter.tex\", latex_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results_table = best_results.sort_values(by=[\"dataset\"])[[\"dataset\", \"bloom-filter\", \"config_name\", \"model_size_improvement\"]]\n",
    "best_results_table.groupby(\"bloom-filter\")[\"model_size_improvement\"].agg([\"mean\", \"std\", \"count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Select the wisard with best aggregated bloom filter\n",
    "\n",
    "Here we show that, costing up to 1% of performance of the best dict-wisard, a \n",
    "space-efficient bloom filter achieves the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Up to 2% of accuracy loss\n",
    "metric_threshold = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = base_results.copy()\n",
    "best_results = best_metric_dataset_model.copy()\n",
    "\n",
    "results = results[results[\"model\"] == \"wisard\"]\n",
    "best_results = best_results[best_results[\"model\"] == \"wisard\"]\n",
    "\n",
    "results[\"bloom-filter\"] = results[\"config_name\"].apply(lambda x: x.split(\" \")[0])\n",
    "best_results[\"bloom-filter\"] = best_results[\"config_name\"].apply(lambda x: x.split(\" \")[0])\n",
    "\n",
    "# Filter: only Dict, CountingBloomFilter and CountMinSketch\n",
    "results = results[results[\"bloom-filter\"].isin([\"Dict\", \"CountingBloomFilter\", \"CountMinSketch\"])]\n",
    "best_results = best_results[best_results[\"bloom-filter\"].isin([\"Dict\", \"CountingBloomFilter\", \"CountMinSketch\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "\n",
    "\n",
    "for dset_name, dset_df in results.groupby(\"dataset\"):\n",
    "    best_model = best_results[best_results[\"dataset\"] == dset_name].iloc[0]\n",
    "    # print(f\"*** Dataset: {dset_name} with metric: {best_model['metric']} and model size: {int(best_model['model_size'])}\")\n",
    "    bests = dset_df[dset_df[\"metric\"] >= best_model[\"metric\"] - metric_threshold]\n",
    "    bests[\"metric_improvement\"] = bests[\"metric\"]/ best_model[\"metric\"]\n",
    "    bests[\"model_size_improvement\"] = bests[\"model_size\"]/ best_model[\"model_size\"]\n",
    "    bests[\"best_metric\"]  = best_model[\"metric\"]\n",
    "    bests[\"best_model_size\"]  = best_model[\"model_size\"]\n",
    "    temp.append(bests)\n",
    "    \n",
    "results = pd.concat(temp, ignore_index=True)\n",
    "results.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results = results.loc[results.groupby(\"dataset\")[\"model_size_improvement\"].idxmin()]\n",
    "best_results[\"bloom-filter\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results_table = best_results.sort_values(by=[\"dataset\"])[[\"dataset\", \"config_name\", \"model_size_improvement\"]]\n",
    "\n",
    "best_results_table.rename(\n",
    "    columns={\n",
    "        \"dataset\": \"Dataset\",\n",
    "        \"config_name\": \"Bloom Filter Configuration\",\n",
    "        \"model_size_improvement\": \"Size Ratio\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "best_results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_str = best_results_table.to_latex(\n",
    "    index=False,\n",
    "    escape=True,\n",
    "    caption=\"Best Bloom Filter configuration for each dataset\",\n",
    "    label=\"tab:best_bloom_filter_agg\",\n",
    "    float_format=\"%.2f\",\n",
    ")\n",
    "\n",
    "write_latex_table(\"best_bloom_filter_agg.tex\", latex_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results_table = best_results.sort_values(by=[\"dataset\"])[[\"dataset\", \"bloom-filter\", \"config_name\", \"model_size_improvement\"]]\n",
    "best_results_table.groupby(\"bloom-filter\")[\"model_size_improvement\"].agg([\"mean\", \"std\", \"count\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
